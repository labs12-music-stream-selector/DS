{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec and Doc2Vec to get numeric representations of YouTube video\n",
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.7.3)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.8.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (1.9.148)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.148 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.148)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.148->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.148->boto3->smart-open>=1.7.0->gensim) (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "from gensim.models import doc2vec, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_id</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[FREE DL] Dave East Type Beat \"Momma Workin\" (...</td>\n",
       "      <td>kQKLl6jXipQ</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logic - Indica Badu ft. Wiz Khalifa [Instrumen...</td>\n",
       "      <td>-QQUaWtMW3w</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>„ÄêOff Vocal„ÄëYasuha. - I Lost Your Love feat. Ha...</td>\n",
       "      <td>1QejWtVKE8s</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shiva In Exile - Earth Tone (Instrumental)</td>\n",
       "      <td>4zrLieLZ_i8</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8 Remix - Almighty ft. Varios [INSTRUMENTAL + ...</td>\n",
       "      <td>kmLvR68_nLg</td>\n",
       "      <td>CONFIDENT-SASSY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title     video_id  \\\n",
       "0  [FREE DL] Dave East Type Beat \"Momma Workin\" (...  kQKLl6jXipQ   \n",
       "1  Logic - Indica Badu ft. Wiz Khalifa [Instrumen...  -QQUaWtMW3w   \n",
       "2  „ÄêOff Vocal„ÄëYasuha. - I Lost Your Love feat. Ha...  1QejWtVKE8s   \n",
       "3         Shiva In Exile - Earth Tone (Instrumental)  4zrLieLZ_i8   \n",
       "4  8 Remix - Almighty ft. Varios [INSTRUMENTAL + ...  kmLvR68_nLg   \n",
       "\n",
       "             moods  \n",
       "0            CHILL  \n",
       "1            HAPPY  \n",
       "2            CHILL  \n",
       "3            CHILL  \n",
       "4  CONFIDENT-SASSY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df = pd.read_csv('new_videos.csv')  # note that this csv data does not have comments column\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2raUmsKIG6U</td>\n",
       "      <td>['That is some cool grafiti song, sick!', 'Dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XAkH0w3QLc</td>\n",
       "      <td>['the \"jake young\" at the begining scared me l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hXe0Elz7WV8</td>\n",
       "      <td>['This is good. Definitely on favorite song li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x1frIzjJv5E</td>\n",
       "      <td>['I think I would sound better at the original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPqaRRdb7eQ</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                           comments\n",
       "0  2raUmsKIG6U  ['That is some cool grafiti song, sick!', 'Dam...\n",
       "1  3XAkH0w3QLc  ['the \"jake young\" at the begining scared me l...\n",
       "2  hXe0Elz7WV8  ['This is good. Definitely on favorite song li...\n",
       "3  x1frIzjJv5E  ['I think I would sound better at the original...\n",
       "4  iPqaRRdb7eQ                                                 []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('new_comments.csv', encoding=\"utf-8\")  # note no moods column\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_videos_df = pd.merge(videos_df, comments_df, on='video_id')\n",
    "# use above line as example, if need to merge new_comments and new_videos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and tokenizing the comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'That is some cool grafiti song, sick!\\', \\'Damn this is dope, I miss the 90s\\', \\'Nice beat and cuts man, unfortunately they faded so early.\\', \\'Very nice beat bra\\', \\'Can you feel IT can you feel IT can you feeeeel itttt\\', \\'this is super golden age hip hop, but still so atmospheric with that string sample!! i just dropped a new vid on my channel would love to hear the opinion of a true artist like you over there! love this beat tho bro!\\', \\'Gj /GoodJob!\\', \\'Se puede sacar una super improvisacion buen beat\\', \"Hello. I want to use ur music for a dance project but i can\\'t find you on facebook, and also not on soundcloud. How can i get your music? Thank you\", \"who\\'s down to start a rap group\", \\'You bouta get the cred for some slick shit bro this beat is fucking absurd\\', \"It\\'s ok if not\", \\'Can I use this brother?\\', \\'Hola man queria saber si podia utilizar este  beat para un tema que tengo pensado (se llama \"mientras suenan las sirenas ) y es justo lo que me imagine para el tema queria saber si tenia tu okey y obiamente siguiendo lo q dice en la descripcion de todos modos gracias y muy buen beat\\', \\'Great\\', \\'Amazing beatüî•\\', \\'Can I use this beat\\\\nI will give you credit also\\', \\'brilliant\\', \"Bro it\\'s the best ever beat i have ever heard\", \\'nice work bro, check my new wc beat: https://goo.gl/87Ueiw\\']'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the raw comments for the first video\n",
    "first_vid_comments = comments_df.comments.values[0]\n",
    "first_vid_comments  # NOTE: this whole object is a string; change it to a list with strings inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried various operations on first video comments\n",
    "# Create a list object of first video comments by splitting on \"\\', \" -- not successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'the \"jake young\" at the begining scared me lolol\\', \\'oml there are literally 4-5 comments with lyrics i cant-\\', \\'NNA NANANA NANAN NANANNAANNNAANANANANANANANANANANANNNNANANANANANANANANAN*YUMINOSHIi?b0AI\\', \\'Chi √® qui per Klaus? üáÆüáπüáÆüáπüáÆüáπ\\', \\'great job, can I use it for a cover? :) thank you\\', \\'Shut your mouth, I don‚Äôt wanna fuckin breath now\\\\nAll the people on my back I‚Äôm getting weighed down \\\\nMy future‚Äôs falling to the floor, I‚Äôm getting worried now\\\\nWill I make it in this game or will I fuckin drown\\\\n\\\\nI can‚Äôt fuckin sleep\\\\nI can‚Äôt fuckin breath \\\\nAll these pills are on my mind, now I‚Äôm in this deep\\\\nFinally passed out on the floor from all this codeine \\\\nShe be screaming I want more until she fuck me\\\\n\\\\nLife‚Äôs a bitter mess when you are depressed \\\\nAll these demons in my head I‚Äôm feeling possessed\\\\nI got problems with these women, yeah I‚Äôm obsessed \\\\nEvery day I‚Äôm working hard creating progress \\\\n\\\\nWhen I wake up and I know you‚Äôre on my fucking mind \\\\nI can‚Äôt face it, you just take up all my fucking time\\\\nI‚Äôm taking chemicals to get me through this fuckin life \\\\nI say I‚Äôm good and she know it‚Äôs a fucking lie\\\\n\\\\nWill they ever listen to me \\\\nYou wanna lead and I wanna leave \\\\nWanna feel like you‚Äôre fuckin free\\\\nWell here you go, baby take my sleep\\\\n\\\\nWhen I wake up and I know you‚Äôre on my fucking mind \\\\nI can‚Äôt face it, you just take up all my fucking time\\\\nI‚Äôm taking chemicals to get me through this fuckin life \\\\nI say I‚Äôm good and she know it‚Äôs a fucking lie\\', \\'Hi! would also like to use your music for a cover. is that cool? Will credit you :)\\', \\'Awesome work‚ù§ also, thank you sooo much for putting the sheets available! üòä\\', \\'CAN I USE THIS FOR A COVER? üòçüôãüèº\\\\u200d‚ôÇÔ∏è\\', \\'This is so gorgeous. Thank you\\', \\'Gard bless you for this!\\', \\'can I use this for my video?\\', \\'Beautiful!\\\\nThe only thing you\\\\\\'re missing is that one part with the \"You caused it\", I can\\\\\\'t describe it. But it\\\\\\'s accompanied by a guitar you can only hear in your right ear. I hope you understand. But it\\\\\\'s beautiful regardless. I\\\\\\'d just love it even more if you include it.\\', \\'could you do Already Gone by Sleeping At Last? there are no instrumentals out for it it would be amazing :) thanks x\\', \"Would it be okay to use this for my cover? I\\'ll give credit :D\", \\'Wonderful ... and hopeful.\\', \\'Thank the lord this exists.\\', \\'may i use this for a cover ??\\', \\'I would love to do a cover of this song. I will give credit of course :3\\', \\'Hey can I choreograph a dance to this for a show at the end of this semester? I‚Äôll credit you\\']'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the raw comments for the second video\n",
    "second_vid_comments = comments_df.comments.values[1]\n",
    "second_vid_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study all comments for all videos\n",
    "# Based on methods from aws-sagemaker-python-notebook [not yet updated on github]\n",
    "all_comments = []\n",
    "all_comments_dict = {}\n",
    "for comment_group in list(comments_df.comments):\n",
    "    comment_group = comment_group.replace('[', '')\n",
    "    comment_group = comment_group.replace(']', '')\n",
    "    comment_group = comment_group.replace(\"'\", '')\n",
    "    all_comments.append(comment_group)\n",
    "    # if ', ' in comment_group:\n",
    "    #     comment_group = comment_group.split(', ')\n",
    "    #     for item in comment_group:\n",
    "    #         all_comments.append(comment_group)\n",
    "    # print(comment_group, len(comment_group))\n",
    "\n",
    "print('length of all_comments is:', len(all_comments))\n",
    "print(all_comments[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lower-case, do several char replacements, and create list by splitting comments string\n",
    "# Create a function to do this when applied to all values in df's comments column\n",
    "\n",
    "def normalize_text(comment_group):\n",
    "    norm_comment_group = comment_group.lower()\n",
    "    norm_comment_group = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_comment_group)\n",
    "    norm_comment_group = norm_comment_group.replace('\\\\n', ', ')\n",
    "    norm_comment_group = norm_comment_group.replace('\"', \"\\'\")\n",
    "    while \"\\'\" in norm_comment_group:\n",
    "        norm_comment_group = norm_comment_group.replace(\"\\'\", '')\n",
    "    norm_comment_group = norm_comment_group.replace('[', '')\n",
    "    norm_comment_group = norm_comment_group.replace(']', '')\n",
    "    return norm_comment_group\n",
    "\n",
    "# Apply normalize_text function to all comments column values\n",
    "\n",
    "comments_df.comments = comments_df.comments.apply(lambda x: normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['that is some cool grafiti song ,  sick !  ,  damn this is dope ,  i miss the 90s ,  nice beat and cuts man ,  unfortunately they faded so early .  ,  very nice beat bra ,  can you feel it can you feel it can you feeeeel itttt ,  this is super golden age hip hop ,  but still so atmospheric with that string sample !  !  i just dropped a new vid on my channel would love to hear the opinion of a true artist like you over there !  love this beat tho bro !  ,  gj /goodjob !  ,  se puede sacar una super improvisacion buen beat ,    hello .  i want to use ur music for a dance project but i cant find you on facebook ,  and also not on soundcloud .  how can i get your music ?  thank you   ,    whos down to start a rap group   ,  you bouta get the cred for some slick shit bro this beat is fucking absurd ,    its ok if not   ,  can i use this brother ?  ,  hola man queria saber si podia utilizar este  beat para un tema que tengo pensado  ( se llama   mientras suenan las sirenas  )  y es justo lo que me imagine para el tema queria saber si tenia tu okey y obiamente siguiendo lo q dice en la descripcion de todos modos gracias y muy buen beat ,  great ,  amazing beatüî• ,  can i use this beat, i will give you credit also ,  brilliant ,    bro its the best ever beat i have ever heard   ,  nice work bro ,  check my new wc beat :  https : //goo . gl/87ueiw',\n",
       "       'the   jake young   at the begining scared me lolol ,  oml there are literally 4-5 comments with lyrics i cant- ,  nna nanana nanan nanannaannnaananananananananananannnnananananananananan*yuminoshii ? b0ai ,  chi √® qui per klaus ?  üáÆüáπüáÆüáπüáÆüáπ ,  great job ,  can i use it for a cover ?   :  )  thank you ,  shut your mouth ,  i don‚Äôt wanna fuckin breath now, all the people on my back i‚Äôm getting weighed down , my future‚Äôs falling to the floor ,  i‚Äôm getting worried now, will i make it in this game or will i fuckin drown, , i can‚Äôt fuckin sleep, i can‚Äôt fuckin breath , all these pills are on my mind ,  now i‚Äôm in this deep, finally passed out on the floor from all this codeine , she be screaming i want more until she fuck me, , life‚Äôs a bitter mess when you are depressed , all these demons in my head i‚Äôm feeling possessed, i got problems with these women ,  yeah i‚Äôm obsessed , every day i‚Äôm working hard creating progress , , when i wake up and i know you‚Äôre on my fucking mind , i can‚Äôt face it ,  you just take up all my fucking time, i‚Äôm taking chemicals to get me through this fuckin life , i say i‚Äôm good and she know it‚Äôs a fucking lie, , will they ever listen to me , you wanna lead and i wanna leave , wanna feel like you‚Äôre fuckin free, well here you go ,  baby take my sleep, , when i wake up and i know you‚Äôre on my fucking mind , i can‚Äôt face it ,  you just take up all my fucking time, i‚Äôm taking chemicals to get me through this fuckin life , i say i‚Äôm good and she know it‚Äôs a fucking lie ,  hi !  would also like to use your music for a cover .  is that cool ?  will credit you  :  )  ,  awesome work‚ù§ also ,  thank you sooo much for putting the sheets available !  üòä ,  can i use this for a cover ?  üòçüôãüèº\\\\u200d‚ôÇÔ∏è ,  this is so gorgeous .  thank you ,  gard bless you for this !  ,  can i use this for my video ?  ,  beautiful ! , the only thing you\\\\re missing is that one part with the   you caused it   ,  i can\\\\t describe it .  but it\\\\s accompanied by a guitar you can only hear in your right ear .  i hope you understand .  but it\\\\s beautiful regardless .  i\\\\d just love it even more if you include it .  ,  could you do already gone by sleeping at last ?  there are no instrumentals out for it it would be amazing  :  )  thanks x ,    would it be okay to use this for my cover ?  ill give credit  : d   ,  wonderful  .  .  .  and hopeful .  ,  thank the lord this exists .  ,  may i use this for a cover  ?  ?  ,  i would love to do a cover of this song .  i will give credit of course  : 3 ,  hey can i choreograph a dance to this for a show at the end of this semester ?  i‚Äôll credit you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.comments.values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The fourth video, with id iPqaRRdb7eQ, has no comments. \n",
    "comments_df.comments.values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(sent_1)  # Example of using NLTK to tokenize one video's comments as one sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying TextBlob for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (3.4.0.3)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags can be removed if in nltk stopwords\n",
    "# To be able to iterate over stopwords, have to put them in an iterable\n",
    "\n",
    "def tags_filter_stopwords(tags):\n",
    "    stop_eng = set(stopwords.words('english'))\n",
    "    stop_esp = set(stopwords.words('spanish'))\n",
    "    stop_fr = set(stopwords.words('french'))\n",
    "    stop_de = set(stopwords.words('german'))\n",
    "    # no stopwords available for Chinese or Japanese\n",
    "    stop_four = stop_eng.union(stop_esp.union(stop_fr.union(stop_de)))\n",
    "    tags_list = []\n",
    "    for tag in tags:\n",
    "        if tag[0] not in stop_four:\n",
    "            tags_list.append(tag)\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "# Also want to know which parts of speech TextBlob codes are most common among the tags\n",
    "def top_three_tag_codes(tags):\n",
    "    \"\"\"Receives list of tags; returns 3 most common TextBlob parts-of-speech codes\"\"\"\n",
    "    # Recall that each TextBlob tag is a tuple of a word and a TextBlob part-of-speech code\n",
    "    speech_parts_list = []\n",
    "    for tag in tags:\n",
    "        speech_parts_list.append(tag[1])\n",
    "    c = Counter(speech_parts_list)\n",
    "    print(c.most_common(3), 'len(c.most_common(3)) is:', len(c.most_common(3)))\n",
    "    # note that there may only be one most common tag\n",
    "    return c.most_common(3)  # returns a list of 2-item tuple[s] with code and count for code in tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two new columns to comments_df: one each for TextBlob polarity mean and subjectivity mean\n",
    "# Want to add other new columns also based on tags:\n",
    "# one column for each of the top 3, and one column for the count in comments for each of the top 3 \n",
    "\n",
    "tags_columns = ['polarity_mean', 'subjectivity_mean', 'tags', 'top_tag', 'top_tag_count',\n",
    "                'second_tag', 'second_tag_count', 'third_tag', 'third_tag_count']\n",
    "analysis_df = pd.concat([comments_df, pd.DataFrame(columns=tags_columns)], axis=1)\n",
    "\n",
    "analysis_df = pd.merge(analysis_df, videos_df, on='video_id')\n",
    "cols = analysis_df.columns.to_list()\n",
    "cols = [cols[0]] + [cols[11]] + [cols[1]] + cols[2:11] + [cols[-1]]\n",
    "analysis_df = analysis_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <th>tags</th>\n",
       "      <th>top_tag</th>\n",
       "      <th>top_tag_count</th>\n",
       "      <th>second_tag</th>\n",
       "      <th>second_tag_count</th>\n",
       "      <th>third_tag</th>\n",
       "      <th>third_tag_count</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2raUmsKIG6U</td>\n",
       "      <td>[SOLD]\" Graffiti \" - 90s Old School Storytelli...</td>\n",
       "      <td>that is some cool grafiti song ,  sick !  ,  d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XAkH0w3QLc</td>\n",
       "      <td>Youth by Daughter | Instrumental (w/ Sheet Music)</td>\n",
       "      <td>the   jake young   at the begining scared me l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hXe0Elz7WV8</td>\n",
       "      <td>THE CREATOR LIED TO US (Fansong Instrumental) ...</td>\n",
       "      <td>this is good .  definitely on favorite song li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CONFIDENT-SASSY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x1frIzjJv5E</td>\n",
       "      <td>Kendrick Lamar - Blood. (RoadsArt Instrumental...</td>\n",
       "      <td>i think i would sound better at the original s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPqaRRdb7eQ</td>\n",
       "      <td>Nco Qub Hluas Nraug Instrumental + Lyrics - Gi...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  2raUmsKIG6U  [SOLD]\" Graffiti \" - 90s Old School Storytelli...   \n",
       "1  3XAkH0w3QLc  Youth by Daughter | Instrumental (w/ Sheet Music)   \n",
       "2  hXe0Elz7WV8  THE CREATOR LIED TO US (Fansong Instrumental) ...   \n",
       "3  x1frIzjJv5E  Kendrick Lamar - Blood. (RoadsArt Instrumental...   \n",
       "4  iPqaRRdb7eQ  Nco Qub Hluas Nraug Instrumental + Lyrics - Gi...   \n",
       "\n",
       "                                            comments polarity_mean  \\\n",
       "0  that is some cool grafiti song ,  sick !  ,  d...           NaN   \n",
       "1  the   jake young   at the begining scared me l...           NaN   \n",
       "2  this is good .  definitely on favorite song li...           NaN   \n",
       "3  i think i would sound better at the original s...           NaN   \n",
       "4                                                              NaN   \n",
       "\n",
       "  subjectivity_mean tags top_tag top_tag_count second_tag second_tag_count  \\\n",
       "0               NaN  NaN     NaN           NaN        NaN              NaN   \n",
       "1               NaN  NaN     NaN           NaN        NaN              NaN   \n",
       "2               NaN  NaN     NaN           NaN        NaN              NaN   \n",
       "3               NaN  NaN     NaN           NaN        NaN              NaN   \n",
       "4               NaN  NaN     NaN           NaN        NaN              NaN   \n",
       "\n",
       "  third_tag third_tag_count            moods  \n",
       "0       NaN             NaN            CHILL  \n",
       "1       NaN             NaN            CHILL  \n",
       "2       NaN             NaN  CONFIDENT-SASSY  \n",
       "3       NaN             NaN            CHILL  \n",
       "4       NaN             NaN            CHILL  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 13)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where there are no comments\n",
    "analysis_df = analysis_df[analysis_df.comments != '']\n",
    "analysis_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 67), ('JJ', 25), ('VB', 14)] len(c.most_common(3)) is: 3\n",
      "[('NN', 85), ('JJ', 32), ('VBP', 31)] len(c.most_common(3)) is: 3\n",
      "[('NN', 22), ('JJ', 10), ('NNS', 4)] len(c.most_common(3)) is: 3\n",
      "[('NN', 19), ('JJ', 7), ('VB', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 8)] len(c.most_common(3)) is: 1\n",
      "[('NN', 5), ('JJ', 4), ('IN', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 5), ('VBD', 1), ('VBP', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 24), ('JJ', 7), ('VB', 5)] len(c.most_common(3)) is: 3\n",
      "[('NN', 224), ('JJ', 39), ('VB', 37)] len(c.most_common(3)) is: 3\n",
      "[('NN', 148), ('VB', 72), ('JJ', 51)] len(c.most_common(3)) is: 3\n",
      "[('NN', 69), ('VB', 23), ('JJ', 23)] len(c.most_common(3)) is: 3\n",
      "[('NN', 131), ('JJ', 66), ('VBP', 65)] len(c.most_common(3)) is: 3\n",
      "[('NN', 273), ('JJ', 73), ('NNS', 53)] len(c.most_common(3)) is: 3\n",
      "[('NN', 8), ('JJ', 3), ('FW', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 5), ('VBP', 3)] len(c.most_common(3)) is: 2\n",
      "[('NN', 172), ('NNS', 60), ('JJ', 51)] len(c.most_common(3)) is: 3\n",
      "[('NN', 70), ('JJ', 25), ('VBP', 13)] len(c.most_common(3)) is: 3\n",
      "[('NN', 59), ('JJ', 13), ('VB', 5)] len(c.most_common(3)) is: 3\n",
      "[('NN', 57), ('JJ', 11), ('VB', 9)] len(c.most_common(3)) is: 3\n",
      "[('NNS', 1), ('VBP', 1)] len(c.most_common(3)) is: 2\n",
      "[('NN', 33), ('JJ', 13), ('VB', 10)] len(c.most_common(3)) is: 3\n",
      "[('NN', 113), ('JJ', 47), ('VB', 36)] len(c.most_common(3)) is: 3\n",
      "[('VB', 3), ('NN', 1), ('NNS', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 159), ('VB', 53), ('JJ', 46)] len(c.most_common(3)) is: 3\n",
      "[('NN', 465), ('VBD', 111), ('VB', 99)] len(c.most_common(3)) is: 3\n",
      "[('NN', 10), ('JJ', 6), ('VB', 2)] len(c.most_common(3)) is: 3\n",
      "[('NN', 53), ('JJ', 22), ('VB', 12)] len(c.most_common(3)) is: 3\n",
      "[('NN', 32), ('NNP', 21), ('JJ', 10)] len(c.most_common(3)) is: 3\n",
      "[('VB', 78), ('NN', 69), ('VBP', 33)] len(c.most_common(3)) is: 3\n",
      "[('NN', 91), ('JJ', 21), ('VBD', 19)] len(c.most_common(3)) is: 3\n",
      "[('NN', 59), ('JJ', 27), ('VB', 11)] len(c.most_common(3)) is: 3\n",
      "[('NN', 141), ('JJ', 28), ('NNP', 8)] len(c.most_common(3)) is: 3\n",
      "[('NN', 706), ('JJ', 187), ('FW', 47)] len(c.most_common(3)) is: 3\n",
      "[('NN', 8), ('CD', 2), ('VB', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 142), ('JJ', 67), ('VB', 64)] len(c.most_common(3)) is: 3\n",
      "[('JJ', 16), ('NN', 11), ('VB', 5)] len(c.most_common(3)) is: 3\n",
      "[('NN', 11)] len(c.most_common(3)) is: 1\n",
      "[('NN', 40), ('JJ', 20), ('VB', 13)] len(c.most_common(3)) is: 3\n",
      "[('NN', 4), ('JJ', 2), ('JJS', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 43), ('NNP', 32), ('JJ', 14)] len(c.most_common(3)) is: 3\n",
      "[('NN', 24), ('JJ', 8), ('NNS', 5)] len(c.most_common(3)) is: 3\n",
      "[('NN', 133), ('VB', 77), ('VBP', 69)] len(c.most_common(3)) is: 3\n",
      "[('NN', 8), ('NNS', 1), ('JJ', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 33), ('JJ', 18), ('VBP', 7)] len(c.most_common(3)) is: 3\n",
      "[('NN', 34), ('JJ', 13), ('NNS', 6)] len(c.most_common(3)) is: 3\n",
      "[('NN', 46), ('NNP', 9), ('JJ', 7)] len(c.most_common(3)) is: 3\n",
      "[('JJ', 4), ('NN', 4), ('NNS', 2)] len(c.most_common(3)) is: 3\n",
      "[('NN', 227), ('JJ', 32), ('NNS', 15)] len(c.most_common(3)) is: 3\n",
      "[('NN', 76), ('JJ', 39), ('VBD', 19)] len(c.most_common(3)) is: 3\n",
      "[('NN', 102), ('VB', 26), ('VBP', 18)] len(c.most_common(3)) is: 3\n",
      "[('NN', 61), ('JJ', 17), ('VB', 8)] len(c.most_common(3)) is: 3\n",
      "[('NN', 63), ('VBP', 24), ('VB', 24)] len(c.most_common(3)) is: 3\n",
      "[('NN', 10), ('NNS', 3), ('VBP', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 152), ('FW', 41), ('JJ', 33)] len(c.most_common(3)) is: 3\n",
      "[('NN', 77), ('VB', 26), ('JJ', 22)] len(c.most_common(3)) is: 3\n",
      "[('NN', 3)] len(c.most_common(3)) is: 1\n",
      "[('NN', 26), ('FW', 6), ('JJ', 4)] len(c.most_common(3)) is: 3\n",
      "[('NN', 88), ('VB', 30), ('JJ', 23)] len(c.most_common(3)) is: 3\n",
      "[('NN', 75), ('FW', 19), ('JJ', 15)] len(c.most_common(3)) is: 3\n",
      "[('NN', 11), ('JJ', 9), ('VB', 4)] len(c.most_common(3)) is: 3\n",
      "[('NN', 116), ('NNS', 34), ('JJ', 31)] len(c.most_common(3)) is: 3\n",
      "[('NN', 9), ('JJ', 6), ('CD', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 2), ('JJ', 1), ('NNS', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 33), ('JJ', 5), ('VB', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 175), ('VB', 84), ('VBP', 76)] len(c.most_common(3)) is: 3\n",
      "[('NN', 202), ('VB', 36), ('VBP', 33)] len(c.most_common(3)) is: 3\n",
      "[('NN', 42), ('JJ', 9), ('VB', 7)] len(c.most_common(3)) is: 3\n",
      "[('NN', 8), ('VBP', 4), ('VB', 3)] len(c.most_common(3)) is: 3\n",
      "[('NN', 12), ('VBG', 3), ('JJ', 2)] len(c.most_common(3)) is: 3\n",
      "[('NN', 44), ('JJ', 20), ('VB', 16)] len(c.most_common(3)) is: 3\n",
      "[('NN', 127), ('JJ', 26), ('FW', 15)] len(c.most_common(3)) is: 3\n",
      "[('NN', 151), ('JJ', 74), ('FW', 21)] len(c.most_common(3)) is: 3\n",
      "[('NN', 4), ('JJ', 2)] len(c.most_common(3)) is: 2\n",
      "[('JJ', 4), ('NN', 4), ('VBP', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 2), ('VB', 2), ('CD', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 76), ('NNP', 38), ('JJ', 14)] len(c.most_common(3)) is: 3\n",
      "[('NN', 34), ('JJ', 7), ('VB', 6)] len(c.most_common(3)) is: 3\n",
      "[('NN', 210), ('JJ', 37), ('FW', 28)] len(c.most_common(3)) is: 3\n",
      "[('NN', 34), ('JJ', 16), ('VB', 7)] len(c.most_common(3)) is: 3\n",
      "[('NNP', 19), ('NN', 10), ('CD', 5)] len(c.most_common(3)) is: 3\n",
      "[('NN', 134), ('NNP', 100), ('JJ', 51)] len(c.most_common(3)) is: 3\n",
      "[('NN', 331), ('JJ', 271), ('VBP', 6)] len(c.most_common(3)) is: 3\n",
      "[('NN', 7), ('NNS', 2), ('VBP', 2)] len(c.most_common(3)) is: 3\n",
      "[('NN', 13), ('JJ', 2), ('UH', 1)] len(c.most_common(3)) is: 3\n",
      "[('NN', 100), ('JJ', 43), ('VB', 32)] len(c.most_common(3)) is: 3\n",
      "[('NN', 56), ('JJ', 23), ('VBD', 9)] len(c.most_common(3)) is: 3\n"
     ]
    }
   ],
   "source": [
    "# Add TextBlob mean polarity, mean subjectivity, and tags features for all comments for each video\n",
    "# Write functions to do each of the following:\n",
    "# 1) run TextBlob on comments; 2) populate polarity_mean, subjectivity_mean, and tags values\n",
    "\n",
    "def get_blob(comments):\n",
    "    \"\"\"Use with one video's comments at a time.\"\"\"\n",
    "    comments = normalize_text(comments)\n",
    "    blob = TextBlob(comments)\n",
    "    tags = tags_filter_stopwords(blob.tags)\n",
    "    top_tags = top_three_tag_codes(tags)  # this is a list of 2-item tuple[s]\n",
    "    top_tag = top_tags[0][0]\n",
    "    top_tag_count = top_tags[0][1]\n",
    "    if len(top_tags) < 2:\n",
    "        second_tag = np.nan\n",
    "        second_tag_count = 0\n",
    "        third_tag = np.nan\n",
    "        third_tag_count = 0\n",
    "    elif len(top_tags) == 2:\n",
    "        second_tag = top_tags[1][0]\n",
    "        second_tag_count = top_tags[1][1]\n",
    "        third_tag = np.nan\n",
    "        third_tag_count = 0\n",
    "    elif len(top_tags) > 2:\n",
    "        # 3-item list of tuples with each top code and its count\n",
    "        second_tag = top_tags[1][0]\n",
    "        second_tag_count = top_tags[1][1]\n",
    "        third_tag = top_tags[2][0]\n",
    "        third_tag_count = top_tags[2][1]\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "    for sentence in blob.sentences:\n",
    "        polarity = sentence.sentiment.polarity\n",
    "        subjectivity = sentence.sentiment.subjectivity\n",
    "        if polarity !=0 :\n",
    "            polarity_list.append(polarity)\n",
    "        if subjectivity !=0 :\n",
    "            subjectivity_list.append(subjectivity)\n",
    "    if len(polarity_list) == 0:\n",
    "        polarity_list.append(0)\n",
    "    if len(subjectivity_list) == 0:\n",
    "        subjectivity_list.append(0)\n",
    "    # print(top_tag, second_tag)\n",
    "    return (mean(polarity_list), mean(subjectivity_list), tags, top_tag, top_tag_count,\n",
    "            second_tag, second_tag_count, third_tag, third_tag_count)\n",
    "\n",
    "def populate_blob_columns(comms):\n",
    "    \"\"\"Use to fill the polarity_mean, subjectivity_mean, and tags columns.\"\"\"\n",
    "    (polarity_mean, subjectivity_mean, tags, top_tag, top_tag_count,\n",
    "            second_tag, second_tag_count, third_tag, third_tag_count) = get_blob(comms)\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'polarity_mean'] = polarity_mean\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'subjectivity_mean'] = subjectivity_mean\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'tags'] = [tags]\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'top_tag'] = top_tag\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'top_tag_count'] = top_tag_count\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'second_tag'] = second_tag\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'second_tag_count'] = second_tag_count\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'third_tag'] = third_tag\n",
    "    analysis_df.loc[(analysis_df.comments == comms), 'third_tag_count'] = third_tag_count\n",
    "    \n",
    "    \n",
    "for comments in analysis_df.comments.values:\n",
    "    populate_blob_columns(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 13)\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <th>tags</th>\n",
       "      <th>top_tag</th>\n",
       "      <th>top_tag_count</th>\n",
       "      <th>second_tag</th>\n",
       "      <th>second_tag_count</th>\n",
       "      <th>third_tag</th>\n",
       "      <th>third_tag_count</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2raUmsKIG6U</td>\n",
       "      <td>[SOLD]\" Graffiti \" - 90s Old School Storytelli...</td>\n",
       "      <td>that is some cool grafiti song ,  sick !  ,  d...</td>\n",
       "      <td>0.252059</td>\n",
       "      <td>0.652953</td>\n",
       "      <td>[(cool, JJ), (grafiti, NN), (song, NN), (sick,...</td>\n",
       "      <td>NN</td>\n",
       "      <td>67</td>\n",
       "      <td>JJ</td>\n",
       "      <td>25</td>\n",
       "      <td>VB</td>\n",
       "      <td>14</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XAkH0w3QLc</td>\n",
       "      <td>Youth by Daughter | Instrumental (w/ Sheet Music)</td>\n",
       "      <td>the   jake young   at the begining scared me l...</td>\n",
       "      <td>0.45385</td>\n",
       "      <td>0.707216</td>\n",
       "      <td>[(jake, NN), (young, JJ), (begining, NN), (sca...</td>\n",
       "      <td>NN</td>\n",
       "      <td>85</td>\n",
       "      <td>JJ</td>\n",
       "      <td>32</td>\n",
       "      <td>VBP</td>\n",
       "      <td>31</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hXe0Elz7WV8</td>\n",
       "      <td>THE CREATOR LIED TO US (Fansong Instrumental) ...</td>\n",
       "      <td>this is good .  definitely on favorite song li...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>[(good, JJ), (definitely, RB), (favorite, NN),...</td>\n",
       "      <td>NN</td>\n",
       "      <td>22</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "      <td>NNS</td>\n",
       "      <td>4</td>\n",
       "      <td>CONFIDENT-SASSY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x1frIzjJv5E</td>\n",
       "      <td>Kendrick Lamar - Blood. (RoadsArt Instrumental...</td>\n",
       "      <td>i think i would sound better at the original s...</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.811389</td>\n",
       "      <td>[(think, VBP), (would, MD), (sound, VB), (bett...</td>\n",
       "      <td>NN</td>\n",
       "      <td>19</td>\n",
       "      <td>JJ</td>\n",
       "      <td>7</td>\n",
       "      <td>VB</td>\n",
       "      <td>3</td>\n",
       "      <td>CHILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qL034NgzGxY</td>\n",
       "      <td>Cim Ntsoov Cia Instrumental + Lyrics</td>\n",
       "      <td>zoo heev li kho siab heev zoo mloog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(zoo, NN), (heev, NN), (li, NN), (kho, NN), (...</td>\n",
       "      <td>NN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  2raUmsKIG6U  [SOLD]\" Graffiti \" - 90s Old School Storytelli...   \n",
       "1  3XAkH0w3QLc  Youth by Daughter | Instrumental (w/ Sheet Music)   \n",
       "2  hXe0Elz7WV8  THE CREATOR LIED TO US (Fansong Instrumental) ...   \n",
       "3  x1frIzjJv5E  Kendrick Lamar - Blood. (RoadsArt Instrumental...   \n",
       "5  qL034NgzGxY               Cim Ntsoov Cia Instrumental + Lyrics   \n",
       "\n",
       "                                            comments polarity_mean  \\\n",
       "0  that is some cool grafiti song ,  sick !  ,  d...      0.252059   \n",
       "1  the   jake young   at the begining scared me l...       0.45385   \n",
       "2  this is good .  definitely on favorite song li...         0.625   \n",
       "3  i think i would sound better at the original s...        0.2875   \n",
       "5                zoo heev li kho siab heev zoo mloog             0   \n",
       "\n",
       "  subjectivity_mean                                               tags  \\\n",
       "0          0.652953  [(cool, JJ), (grafiti, NN), (song, NN), (sick,...   \n",
       "1          0.707216  [(jake, NN), (young, JJ), (begining, NN), (sca...   \n",
       "2          0.733333  [(good, JJ), (definitely, RB), (favorite, NN),...   \n",
       "3          0.811389  [(think, VBP), (would, MD), (sound, VB), (bett...   \n",
       "5                 0  [(zoo, NN), (heev, NN), (li, NN), (kho, NN), (...   \n",
       "\n",
       "  top_tag top_tag_count second_tag second_tag_count third_tag third_tag_count  \\\n",
       "0      NN            67         JJ               25        VB              14   \n",
       "1      NN            85         JJ               32       VBP              31   \n",
       "2      NN            22         JJ               10       NNS               4   \n",
       "3      NN            19         JJ                7        VB               3   \n",
       "5      NN             8        NaN                0       NaN               0   \n",
       "\n",
       "             moods  \n",
       "0            CHILL  \n",
       "1            CHILL  \n",
       "2  CONFIDENT-SASSY  \n",
       "3            CHILL  \n",
       "5              SAD  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how df looks\n",
    "print(analysis_df.shape)\n",
    "print(analysis_df.isna().sum().sum())\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.205943362193363, 45.38504088504089], dtype=object)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First normalize the data by multiplying polarity_mean and subjectivity_mean each by 100\n",
    "analysis_df.polarity_mean *= 100\n",
    "analysis_df.subjectivity_mean *= 100\n",
    "\n",
    "# Verify that the values have been scaled by 100\n",
    "analysis_df.polarity_mean.values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, define features for X, and target for y\n",
    "# And split data into training and test sets\n",
    "\n",
    "X = analysis_df[['polarity_mean', 'subjectivity_mean', 'top_tag_count',\n",
    "                 'second_tag_count', 'third_tag_count']]\n",
    "y = analysis_df.moods\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model - this is a multiclass classification, supervised machine learning problem\n",
    "# because there are 6 moods in the df\n",
    "\n",
    "# Try a simple model first - KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of the model, using X_test and y_test\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print('accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now will try a more complex model - Random Forest classification\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=25, max_depth=2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
       "            oob_score=False, random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31251438 0.09514156 0.24587961 0.19035721 0.15610723]\n"
     ]
    }
   ],
   "source": [
    "# See the respective importance for each of the features in X\n",
    "print(RFC.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.13636363636363635\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of the model\n",
    "accuracy = RFC.score(X_test, y_test)\n",
    "print('accuracy is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a csv\n",
    "analysis_df.to_csv('model_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox for working with new comments, TextBlob, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with the first video's comments, to start\n",
    "\n",
    "blob = TextBlob(comments_df.comments.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('that', 'DT'), ('is', 'VBZ'), ('some', 'DT')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if tags can be gotten for the first video's comments\n",
    "print(len(blob.tags))\n",
    "blob.tags[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cool', 'JJ'),\n",
       " ('grafiti', 'NN'),\n",
       " ('song', 'NN'),\n",
       " ('sick', 'NN'),\n",
       " ('damn', 'NN'),\n",
       " ('dope', 'NN'),\n",
       " ('miss', 'VBP'),\n",
       " ('90s', 'CD'),\n",
       " ('nice', 'JJ'),\n",
       " ('beat', 'NN'),\n",
       " ('cuts', 'NNS'),\n",
       " ('unfortunately', 'RB'),\n",
       " ('faded', 'VBD'),\n",
       " ('early', 'JJ'),\n",
       " ('nice', 'JJ'),\n",
       " ('beat', 'NN'),\n",
       " ('bra', 'NN'),\n",
       " ('feel', 'VB'),\n",
       " ('feel', 'VB'),\n",
       " ('feeeeel', 'VB'),\n",
       " ('itttt', 'RB'),\n",
       " ('super', 'JJ'),\n",
       " ('golden', 'JJ'),\n",
       " ('age', 'NN'),\n",
       " ('hip', 'NN'),\n",
       " ('hop', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('atmospheric', 'JJ'),\n",
       " ('string', 'VBG'),\n",
       " ('sample', 'NN'),\n",
       " ('dropped', 'VBD'),\n",
       " ('new', 'JJ'),\n",
       " ('vid', 'NN'),\n",
       " ('channel', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('love', 'VB'),\n",
       " ('hear', 'VB'),\n",
       " ('opinion', 'NN'),\n",
       " ('true', 'JJ'),\n",
       " ('artist', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('love', 'VB'),\n",
       " ('beat', 'NN'),\n",
       " ('tho', 'JJ'),\n",
       " ('bro', 'NN'),\n",
       " ('gj', 'JJ'),\n",
       " ('/goodjob', 'NN'),\n",
       " ('puede', 'NN'),\n",
       " ('sacar', 'NN'),\n",
       " ('super', 'JJ'),\n",
       " ('improvisacion', 'NN'),\n",
       " ('buen', 'NN'),\n",
       " ('beat', 'NN'),\n",
       " ('hello', 'NN'),\n",
       " ('want', 'VBP'),\n",
       " ('use', 'VB'),\n",
       " ('ur', 'JJ'),\n",
       " ('music', 'NN'),\n",
       " ('dance', 'NN'),\n",
       " ('project', 'NN'),\n",
       " ('cant', 'VBP'),\n",
       " ('find', 'VBP'),\n",
       " ('facebook', 'NN'),\n",
       " ('soundcloud', 'NN'),\n",
       " ('get', 'VB'),\n",
       " ('music', 'NN'),\n",
       " ('thank', 'NN'),\n",
       " ('whos', 'VB'),\n",
       " ('start', 'VB'),\n",
       " ('rap', 'NN'),\n",
       " ('group', 'NN'),\n",
       " ('bouta', 'VBP'),\n",
       " ('get', 'VB'),\n",
       " ('cred', 'NN'),\n",
       " ('slick', 'JJ'),\n",
       " ('shit', 'NN'),\n",
       " ('bro', 'NN'),\n",
       " ('beat', 'NN'),\n",
       " ('fucking', 'VBG'),\n",
       " ('absurd', 'RB'),\n",
       " ('ok', 'NN'),\n",
       " ('use', 'NN'),\n",
       " ('brother', 'NN'),\n",
       " ('hola', 'NN'),\n",
       " ('queria', 'VBZ'),\n",
       " ('saber', 'JJ'),\n",
       " ('si', 'JJ'),\n",
       " ('podia', 'NN'),\n",
       " ('utilizar', 'JJ'),\n",
       " ('beat', 'NN'),\n",
       " ('tema', 'NN'),\n",
       " ('pensado', 'NN'),\n",
       " ('llama', 'NN'),\n",
       " ('mientras', 'NNS'),\n",
       " ('suenan', 'VBP'),\n",
       " ('sirenas', 'NNS'),\n",
       " ('justo', 'NN'),\n",
       " ('imagine', 'VBP'),\n",
       " ('tema', 'NN'),\n",
       " ('queria', 'NNS'),\n",
       " ('saber', 'VBP'),\n",
       " ('si', 'JJ'),\n",
       " ('tenia', 'NN'),\n",
       " ('okey', 'JJ'),\n",
       " ('obiamente', 'NN'),\n",
       " ('siguiendo', 'NN'),\n",
       " ('q', 'NN'),\n",
       " ('dice', 'NN'),\n",
       " ('descripcion', 'NN'),\n",
       " ('modos', 'FW'),\n",
       " ('gracias', 'FW'),\n",
       " ('buen', 'NN'),\n",
       " ('beat', 'NN'),\n",
       " ('great', 'JJ'),\n",
       " ('amazing', 'JJ'),\n",
       " ('beatüî•', 'NN'),\n",
       " ('use', 'NN'),\n",
       " ('beat', 'NN'),\n",
       " ('give', 'VB'),\n",
       " ('credit', 'NN'),\n",
       " ('brilliant', 'JJ'),\n",
       " ('bro', 'VB'),\n",
       " ('best', 'JJS'),\n",
       " ('ever', 'RB'),\n",
       " ('beat', 'VBP'),\n",
       " ('ever', 'RB'),\n",
       " ('heard', 'VBN'),\n",
       " ('nice', 'JJ'),\n",
       " ('work', 'NN'),\n",
       " ('bro', 'NN'),\n",
       " ('check', 'VB'),\n",
       " ('new', 'JJ'),\n",
       " ('wc', 'JJ'),\n",
       " ('beat', 'NN'),\n",
       " ('https', 'NN'),\n",
       " ('//goo', 'NN'),\n",
       " ('gl/87ueiw', 'NN')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_filter_stopwords(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 81), ('JJ', 32), ('DT', 18)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify correct functioning of top_three_tag_codes\n",
    "top_three_tag_codes(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring sentiment delivered by TextBlob, on 15 May 2019\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "polarity_list = []\n",
    "subjectivity_list = []\n",
    "for sentence in blob.sentences:\n",
    "    print('polarity and subjectivity for this sentence are:', sentence.sentiment)\n",
    "    polarity = sentence.sentiment.polarity\n",
    "    subjectivity = sentence.sentiment.subjectivity\n",
    "    if polarity !=0 :\n",
    "        polarity_list.append(polarity)\n",
    "    if subjectivity !=0 :\n",
    "        subjectivity_list.append(subjectivity)\n",
    "print('polarity_list is:', polarity_list, '\\n', 'subjectivity_list is:', subjectivity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a new comments csv from 16 May 2019\n",
    "\n",
    "comments1_df = pd.read_csv('new_comments1.csv')\n",
    "comments1_df.head()\n",
    "len(comments1_df.loc[comments1_df.comments == '[]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2520594336219336,\n",
       " 0.6529527417027416,\n",
       " [('that', 'DT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('some', 'DT'),\n",
       "  ('cool', 'JJ'),\n",
       "  ('grafiti', 'NN'),\n",
       "  ('song', 'NN'),\n",
       "  ('sick', 'NN'),\n",
       "  ('damn', 'NN'),\n",
       "  ('this', 'DT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('dope', 'NN'),\n",
       "  ('i', 'JJ'),\n",
       "  ('miss', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('90s', 'CD'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('beat', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('cuts', 'NNS'),\n",
       "  ('man', 'NN'),\n",
       "  ('unfortunately', 'RB'),\n",
       "  ('they', 'PRP'),\n",
       "  ('faded', 'VBD'),\n",
       "  ('so', 'RB'),\n",
       "  ('early', 'JJ'),\n",
       "  ('very', 'RB'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('beat', 'NN'),\n",
       "  ('bra', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('you', 'PRP'),\n",
       "  ('feel', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('you', 'PRP'),\n",
       "  ('feel', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('you', 'PRP'),\n",
       "  ('feeeeel', 'VB'),\n",
       "  ('itttt', 'RB'),\n",
       "  ('this', 'DT'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('super', 'JJ'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('age', 'NN'),\n",
       "  ('hip', 'NN'),\n",
       "  ('hop', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('still', 'RB'),\n",
       "  ('so', 'RB'),\n",
       "  ('atmospheric', 'JJ'),\n",
       "  ('with', 'IN'),\n",
       "  ('that', 'DT'),\n",
       "  ('string', 'VBG'),\n",
       "  ('sample', 'NN'),\n",
       "  ('i', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('dropped', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('new', 'JJ'),\n",
       "  ('vid', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('channel', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('love', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('hear', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('opinion', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('true', 'JJ'),\n",
       "  ('artist', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('over', 'IN'),\n",
       "  ('there', 'RB'),\n",
       "  ('love', 'VB'),\n",
       "  ('this', 'DT'),\n",
       "  ('beat', 'NN'),\n",
       "  ('tho', 'JJ'),\n",
       "  ('bro', 'NN'),\n",
       "  ('gj', 'JJ'),\n",
       "  ('/goodjob', 'NN'),\n",
       "  ('se', 'FW'),\n",
       "  ('puede', 'NN'),\n",
       "  ('sacar', 'NN'),\n",
       "  ('una', 'JJ'),\n",
       "  ('super', 'JJ'),\n",
       "  ('improvisacion', 'NN'),\n",
       "  ('buen', 'NN'),\n",
       "  ('beat', 'NN'),\n",
       "  ('hello', 'NN'),\n",
       "  ('i', 'NN'),\n",
       "  ('want', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('use', 'VB'),\n",
       "  ('ur', 'JJ'),\n",
       "  ('music', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('dance', 'NN'),\n",
       "  ('project', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('i', 'JJ'),\n",
       "  ('cant', 'VBP'),\n",
       "  ('find', 'VBP'),\n",
       "  ('you', 'PRP'),\n",
       "  ('on', 'IN'),\n",
       "  ('facebook', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('also', 'RB'),\n",
       "  ('not', 'RB'),\n",
       "  ('on', 'IN'),\n",
       "  ('soundcloud', 'NN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('can', 'MD'),\n",
       "  ('i', 'VB'),\n",
       "  ('get', 'VB'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('music', 'NN'),\n",
       "  ('thank', 'NN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('whos', 'VB'),\n",
       "  ('down', 'RP'),\n",
       "  ('to', 'TO'),\n",
       "  ('start', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('rap', 'NN'),\n",
       "  ('group', 'NN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('bouta', 'VBP'),\n",
       "  ('get', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('cred', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('some', 'DT'),\n",
       "  ('slick', 'JJ'),\n",
       "  ('shit', 'NN'),\n",
       "  ('bro', 'NN'),\n",
       "  ('this', 'DT'),\n",
       "  ('beat', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('fucking', 'VBG'),\n",
       "  ('absurd', 'RB'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('ok', 'NN'),\n",
       "  ('if', 'IN'),\n",
       "  ('not', 'RB'),\n",
       "  ('can', 'MD'),\n",
       "  ('i', 'VB'),\n",
       "  ('use', 'NN'),\n",
       "  ('this', 'DT'),\n",
       "  ('brother', 'NN'),\n",
       "  ('hola', 'NN'),\n",
       "  ('man', 'NN'),\n",
       "  ('queria', 'VBZ'),\n",
       "  ('saber', 'JJ'),\n",
       "  ('si', 'JJ'),\n",
       "  ('podia', 'NN'),\n",
       "  ('utilizar', 'JJ'),\n",
       "  ('este', 'NN'),\n",
       "  ('beat', 'NN'),\n",
       "  ('para', 'NN'),\n",
       "  ('un', 'JJ'),\n",
       "  ('tema', 'NN'),\n",
       "  ('que', 'NN'),\n",
       "  ('tengo', 'NN'),\n",
       "  ('pensado', 'NN'),\n",
       "  ('se', 'JJ'),\n",
       "  ('llama', 'NN'),\n",
       "  ('mientras', 'NNS'),\n",
       "  ('suenan', 'VBP'),\n",
       "  ('las', 'NNS'),\n",
       "  ('sirenas', 'NNS'),\n",
       "  ('y', 'VBP'),\n",
       "  ('es', 'JJ'),\n",
       "  ('justo', 'NN'),\n",
       "  ('lo', 'NN'),\n",
       "  ('que', 'IN'),\n",
       "  ('me', 'PRP'),\n",
       "  ('imagine', 'VBP'),\n",
       "  ('para', 'JJ'),\n",
       "  ('el', 'NN'),\n",
       "  ('tema', 'NN'),\n",
       "  ('queria', 'NNS'),\n",
       "  ('saber', 'VBP'),\n",
       "  ('si', 'JJ'),\n",
       "  ('tenia', 'NN'),\n",
       "  ('tu', 'NN'),\n",
       "  ('okey', 'JJ'),\n",
       "  ('y', 'NN'),\n",
       "  ('obiamente', 'NN'),\n",
       "  ('siguiendo', 'NN'),\n",
       "  ('lo', 'NN'),\n",
       "  ('q', 'NN'),\n",
       "  ('dice', 'NN'),\n",
       "  ('en', 'IN'),\n",
       "  ('la', 'FW'),\n",
       "  ('descripcion', 'NN'),\n",
       "  ('de', 'IN'),\n",
       "  ('todos', 'FW'),\n",
       "  ('modos', 'FW'),\n",
       "  ('gracias', 'FW'),\n",
       "  ('y', 'FW'),\n",
       "  ('muy', 'FW'),\n",
       "  ('buen', 'NN'),\n",
       "  ('beat', 'NN'),\n",
       "  ('great', 'JJ'),\n",
       "  ('amazing', 'JJ'),\n",
       "  ('beatüî•', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('i', 'VB'),\n",
       "  ('use', 'NN'),\n",
       "  ('this', 'DT'),\n",
       "  ('beat', 'NN'),\n",
       "  ('i', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('give', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('credit', 'NN'),\n",
       "  ('also', 'RB'),\n",
       "  ('brilliant', 'JJ'),\n",
       "  ('bro', 'VB'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('the', 'DT'),\n",
       "  ('best', 'JJS'),\n",
       "  ('ever', 'RB'),\n",
       "  ('beat', 'VBP'),\n",
       "  ('i', 'NNS'),\n",
       "  ('have', 'VBP'),\n",
       "  ('ever', 'RB'),\n",
       "  ('heard', 'VBN'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('bro', 'NN'),\n",
       "  ('check', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('new', 'JJ'),\n",
       "  ('wc', 'JJ'),\n",
       "  ('beat', 'NN'),\n",
       "  ('https', 'NN'),\n",
       "  ('//goo', 'NN'),\n",
       "  ('gl/87ueiw', 'NN')])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blob(comments_df.comments.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(comments_df.comments[4])\n",
    "# get_blob(comments_df.comments[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 8)] len(c.most_common(3)) is: 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'second_tag' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-40bd957def37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_blob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-177-26c20d95f83f>\u001b[0m in \u001b[0;36mget_blob\u001b[1;34m(comments)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtop_tag_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_tags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msecond_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_tag_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthird_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthird_tag_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_tags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'second_tag' referenced before assignment"
     ]
    }
   ],
   "source": [
    "get_blob(comments_df.comments[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Doc2Vec NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a Doc2Vec TaggedDocument for each video's comments;\n",
    "# append each of those Doc2Vec TaggedDocuments to a \"sentences\" list;\n",
    "# create: model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1);\n",
    "# call: model.build_vocab(sentences);\n",
    "# train model: for epoch in range(int):... model.train, model.alpha, model.min_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument count in sentences is: 93 \n",
      "\n",
      "TaggedDocument(that is some cool grafiti song ,  sick !  ,  damn this is dope ,  i miss the 90s ,  nice beat and cuts man ,  unfortunately they faded so early .  ,  very nice beat bra ,  can you feel it can you feel it can you feeeeel itttt ,  this is super golden age hip hop ,  but still so atmospheric with that string sample !  !  i just dropped a new vid on my channel would love to hear the opinion of a true artist like you over there !  love this beat tho bro !  ,  gj /goodjob !  ,  se puede sacar una super improvisacion buen beat ,    hello .  i want to use ur music for a dance project but i cant find you on facebook ,  and also not on soundcloud .  how can i get your music ?  thank you   ,    whos down to start a rap group   ,  you bouta get the cred for some slick shit bro this beat is fucking absurd ,    its ok if not   ,  can i use this brother ?  ,  hola man queria saber si podia utilizar este  beat para un tema que tengo pensado  ( se llama   mientras suenan las sirenas  )  y es justo lo que me imagine para el tema queria saber si tenia tu okey y obiamente siguiendo lo q dice en la descripcion de todos modos gracias y muy buen beat ,  great ,  amazing beatüî• ,  can i use this beat, i will give you credit also ,  brilliant ,    bro its the best ever beat i have ever heard   ,  nice work bro ,  check my new wc beat :  https : //goo . gl/87ueiw, ['SENT_0'])\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "sentences_ordered_dict = OrderedDict()\n",
    "sent_count = 0\n",
    "raw_comments = list(comments_df.comments.values)  # this is all of the comments for all videos\n",
    "for comment_group in raw_comments:\n",
    "    sent_key = 'sentence' + str(sent_count)\n",
    "    sentences_ordered_dict[sent_key] = doc2vec.TaggedDocument(\n",
    "        words=comment_group, tags=[\"SENT_\" + str(sent_count)])\n",
    "    sentences.append(sentences_ordered_dict[sent_key])\n",
    "    sent_count += 1\n",
    "print('TaggedDocument count in sentences is:', len(sentences), '\\n')\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc2vec model and build its vocabulary\n",
    "model = Doc2Vec(alpha=.025, min_alpha=.025, min_count=2)\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more info about the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which objects related to vocabulary are available in the Doc2Vec namespace\n",
    "[x for x in dir(model) if 'vocab' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look closer at 'vocabulary'\n",
    "help(model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer new tokens on new comments documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
